{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning technical model for ETFs\n",
    "\n",
    "Based on the kibot data\n",
    "\n",
    "* Based of etf_deep_learning_2020.04.17\n",
    "* Attempt to get minibatch training going --- was not able to in the last version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# preamble\n",
    "%alias_magic  nbagg matplotlib -p nbagg\n",
    "%alias_magic  inline matplotlib -p inline\n",
    "%load_ext memory_profiler\n",
    "\n",
    "from io import BytesIO\n",
    "from IPython.display import display, Image, HTML\n",
    "from pylab import *\n",
    "plt.style.use( 'seaborn-whitegrid' )\n",
    "\n",
    "from madmax.api import *\n",
    "mx = mx.reload()\n",
    "mxtr = mxtr.reload()\n",
    "\n",
    "from research import yahoo; reload( yahoo)\n",
    "\n",
    "###################################################################\n",
    "# Interactive configuration \n",
    "# When running a config grid this will throw a FrozenException \n",
    "# and the grid Config will be used\n",
    "###################################################################\n",
    "try:\n",
    "    mx.Config.name = 'etf_technical/deep_learning/2020.04.27'\n",
    "    mx.Config.mode = 'interactive'\n",
    "    mx.Config.hps = mx.HPSet()\n",
    "    mx.Config.code = 'etf_deep_learning_2020.04.27.ipynb'\n",
    "    disp( 'Running template configuration ', h=2 )        \n",
    "except mx.Config.FrozenException:\n",
    "    if mx.Config.mode != 'grid':\n",
    "        raise RuntimeError( 'An unexpected configruation encountered' )\n",
    "    disp( 'Running a grid configuration ', h=2 )\n",
    "\n",
    "hps = mx.Config.hps\n",
    "mx.Config.start()\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "dtype = tr.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# the various features in the data\n",
    "####################################################################\n",
    "from research.etf_technical import load_data as eld; reload( eld )\n",
    "\n",
    "\n",
    "price_features = ['open_30min', 'high_30min', 'low_30min', 'close_30min', \n",
    "                  'open_day', 'high_day', 'low_day', 'close_day', 'close_prev_day', \n",
    "                  'open_week', 'high_week', 'low_week', 'close_week', 'close_prev_week', \n",
    "                  'open_month', 'high_month', 'low_month', 'close_month', ]\n",
    "                      \n",
    "volume_features = ['volume_30min', 'volume_intraday', 'volume_intraweek', 'volume_1hr', 'volume_1day', \n",
    "                       'dollar_volume_30min', 'dollar_volume_intraday',\n",
    "                       'dollar_volume_intraweek', 'dollar_volume_1hr', 'dollar_volume_1day',]\n",
    "                       \n",
    "calendar_features = [ 'week', 'month', 'timeofday', 'dayofweek', 'dayofmonth', \n",
    "                          'weekofyear', 'cos_timeofday', 'sin_timeofday', \n",
    "                          'cos_dayofweek', 'sin_dayofweek', 'cos_dayofmonth',\n",
    "                          'sin_dayofmonth', 'cos_weekofyear', 'sin_weekofyear',]\n",
    "\n",
    "returns_features = ['logrtn_lag_intraday', 'logrtn_lag_overnight', 'logrtn_lag_intraweek',\n",
    "                    'logrtn_lag_weekend', 'logrtn_lag_intramonth', 'logrtn_lag_30min',\n",
    "                    'logrtn_lag_1hr','logrtn_lag_1day', ]\n",
    "\n",
    "responses = ['logrtn_lead_30min',  'logrtn_lead_1hr', 'logrtn_lead_1day'] #'logrtn_lead_intraday', \n",
    "\n",
    "\n",
    "################################################################################################\n",
    "# build features for the deep learning (same as teh code for the 30-min online model)\n",
    "################################################################################################\n",
    "@mx.operatorize( memoize='md', consumes_features=False, produces_meta=True )\n",
    "def FeatureBuilder( data, responses=responses,  emv=True, ema=True, equalize_lagging_returns=False,\n",
    "                    haar=False, dilation=2, levels=5, winsorize=0.05, smz=False,\n",
    "                    volume_features=volume_features, returns_features=returns_features,\n",
    "                    verbose=False ):\n",
    "    '''\n",
    "    build up the 30 min dataset for learning\n",
    "    \n",
    "    equalize_lagging_returns:\n",
    "        divide out annual vol from all lagging returns features\n",
    "        Leading returns features are equalized anyways\n",
    "    '''\n",
    "    INTS_PER_DAY = 13\n",
    "    logger = mx.Logger( 'build_features', verbose=verbose )\n",
    "    das = []\n",
    "    with logger.timer( 'loading the data'):\n",
    "        for symbol in logger.pbar( eld.kibot_day_files.keys() ):\n",
    "            if symbol.startswith( 'INCOMPLETE' ):\n",
    "                logger.warn( f'No data for {symbol} - SKIPPING' )\n",
    "                continue\n",
    "            logger.info( 'loading data for ', symbol )\n",
    "            da, meta = eld.get_saved_30min_data( symbol=symbol )\n",
    "            # add a mask to indicate when this feature was valid\n",
    "            da = da.assign_features( valid=da.loc[:,:,'open_30min'].isfinite() )\n",
    "            das.append( da )\n",
    "        da = xa.concat( das, dim='symbol' )\n",
    "    features = volume_features + returns_features\n",
    "    with logger.timer( ' compute log features' ):\n",
    "        for ft in ['dollar_volume_30min', 'open_30min', 'dollar_volume_intraday',  'dollar_volume_intraweek',] :\n",
    "            da = da.assign_features( **{ f'log_{ft}': da.loc[:,:, ft].log() } )\n",
    "            features.append( f'log_{ft}' )\n",
    "    with logger.timer(' compute an annual returns volatility'):\n",
    "        avc = mx.transforms.rolling( windows='252D', sid='symbol', min_periods=INTS_PER_DAY*60 ).sd()\n",
    "        da = avc( da, features='logrtn_lag_30min' )\n",
    "        annual_vol = avc.output_features[0]\n",
    "        # equalize the response variable\n",
    "        equalized_responses = []\n",
    "        for response in responses:\n",
    "            equalized_response = f'{response}_equalized'\n",
    "            da = da.assign_features( **{equalized_response: da.loc[:,:,response] / ( da.loc[:,:,annual_vol] + 1e-12 ) } )\n",
    "            equalized_responses.append( equalized_response )\n",
    "    if equalize_lagging_returns:\n",
    "        # equalize all the laggign returns columns inplace ....\n",
    "        with logger.timer( 'compute \"normalized\" leading and lagging returns' ):\n",
    "            da.loc[:,:,returns_features] =  da.loc[:,:,returns_features] / ( da.loc[:,:,annual_vol] + 1e-12 )\n",
    "            # volume, price and returns vol\n",
    "    if emv:\n",
    "        with logger.timer( 'compute volatility' ):\n",
    "            emv = mx.transforms.exponential( halflifes=np.array( [2, 10, 30] ) * INTS_PER_DAY, sid='symbol' ).sd()\n",
    "            da = emv( da, features=['log_open_30min', 'log_dollar_volume_30min', 'logrtn_lag_30min', 'logrtn_lag_1day'] )\n",
    "            features += emv.output_features\n",
    "    # volume, price and returns sma\n",
    "    if ema:\n",
    "        with logger.timer( 'compute ema' ):\n",
    "            ema = mx.transforms.exponential( halflifes=np.array( [2, 10, 30] ) * INTS_PER_DAY, sid='symbol' ).mean()\n",
    "            da = ema( da, features=['log_open_30min', 'log_dollar_volume_30min', 'logrtn_lag_30min', 'logrtn_lag_1day'] )\n",
    "            features += ema.output_features\n",
    "            # compute haar transform\n",
    "    if haar:\n",
    "        with logger.timer( 'compute haar' ):\n",
    "            haar = mx.transforms.haar( levels=levels, dilation=dilation, sid='symbol' )\n",
    "            haar.features = ['log_open_30min', 'log_dollar_volume_30min', 'logrtn_lag_30min', 'logrtn_lag_1day']\n",
    "            da = haar( da )\n",
    "            features += haar.output_features\n",
    "    # winsorize the features we've constructed and other good stuff\n",
    "    if winsorize:\n",
    "        with logger.timer( 'winsorizing' ):\n",
    "            wins = mx.transforms.batch( sid='symbol' ).winsorize( quantiles=winsorize )\n",
    "            wins.features = sorted( set(features + [annual_vol] + equalized_responses  ) )\n",
    "            da = wins( da )\n",
    "            # drop the original features and rename the winsorized one\n",
    "            da = (da.drop_coords( features=wins.features )\n",
    "                  .rename_coords( features=dict( zip( wins.output_features, wins.features ) ) ))\n",
    "    # zscore the features\n",
    "    if smz:\n",
    "        with logger.timer( 'zscoring' ):\n",
    "            zscore = mx.transforms.rolling( windows=smz, sid='symbol' ).zscore()\n",
    "            da = zscore( da, features=features )\n",
    "            # drop the original features and rename the zscored one\n",
    "            da = da.drop_coords( features=zscore.features ).rename_coords( features=dict( zip( zscore.output_features, zscore.features ) ) )\n",
    "    # add a dummy weight column\n",
    "    da = da.assign_features( one=1 )\n",
    "    return da, features, dict( equalized_responses=equalized_responses, annual_vol=annual_vol )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# --- build data nad features ---\n",
    "######################################################################################################\n",
    "builder = FeatureBuilder( responses=responses, volume_features=volume_features, returns_features=returns_features, smz=False  )\n",
    "builder.data = None\n",
    "\n",
    "da = builder()\n",
    "disp( 'expected FeatureBuilder_8f67a6e37 got', builder.hash() )\n",
    "\n",
    "# obtain the metadata for each symbol\n",
    "metas = []\n",
    "for symbol in da.symbol.values:\n",
    "    for etf_list in [eld.bond_etfs, eld.equity_etfs, eld.vol_etfs]:\n",
    "        if symbol in etf_list.Symbol.values:\n",
    "            metas.append( etf_list[etf_list.Symbol == symbol].iloc[0].to_dict() )\n",
    "            break\n",
    "\n",
    "da = da.loc['20100101':]            \n",
    "meta_df =pd.DataFrame(metas)     \n",
    "hps = mx.HPSet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup learning hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the dataset for learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "#setup dataset\n",
    "#-----\n",
    "%nbagg\n",
    "mxtr = mxtr.reload() \n",
    "\n",
    "hps.features =([ \n",
    "    'logrtn_lag_overnight', 'logrtn_lag_weekend', 'logrtn_lag_30min', 'logrtn_lag_1hr', 'logrtn_lag_1day',\n",
    "    'logrtn_lag_intraday', 'logrtn_lag_intraweek',    'logrtn_lag_intramonth',\n",
    "     'logrtn_lag_30min_26.ew_sd',  'logrtn_lag_1day_26.ew_sd',  'logrtn_lag_30min_130.ew_sd', 'logrtn_lag_1day_130.ew_sd',\n",
    "      'logrtn_lag_30min_390.ew_sd', 'logrtn_lag_1day_390.ew_sd','logrtn_lag_30min_26.ew_mean', 'logrtn_lag_1day_26.ew_mean',\n",
    "      'logrtn_lag_30min_130.ew_mean', 'logrtn_lag_1day_130.ew_mean','logrtn_lag_30min_390.ew_mean', 'logrtn_lag_1day_390.ew_mean',\n",
    "     'log_dollar_volume_30min', 'log_dollar_volume_intraday', 'log_dollar_volume_intraweek',\n",
    "     'log_dollar_volume_30min_26.ew_sd',  'log_dollar_volume_30min_130.ew_sd', 'log_dollar_volume_30min_390.ew_sd', \n",
    "     'log_dollar_volume_30min_26.ew_mean',  'log_dollar_volume_30min_130.ew_mean', 'log_dollar_volume_30min_390.ew_mean', \n",
    "     'log_open_30min',\n",
    "     'log_open_30min_26.ew_sd',  'log_open_30min_130.ew_sd',  'log_open_30min_390.ew_sd', 'log_open_30min_26.ew_mean', \n",
    "     'log_open_30min_130.ew_mean', 'log_open_30min_390.ew_mean', \n",
    "]) \n",
    "hps.calendar_features = [] \n",
    "#['cos_timeofday', 'sin_timeofday', 'cos_dayofweek', 'sin_dayofweek', 'cos_dayofmonth','sin_dayofmonth', 'cos_weekofyear', 'sin_weekofyear']\n",
    "hps.responses = [ 'logrtn_lead_1day_equalized'] #['logrtn_lead_30min_equalized', 'logrtn_lead_1hr_equalized', 'logrtn_lead_1day_equalized'] #\n",
    "hps.priming = 13 * 3 * (2 ** 4)  #  priming when computing the loss is related to the dilation and num resnetl laysers\n",
    "hps.data_standardize = False\n",
    "\n",
    "T_da = da.time.astype( np.int64 ) / mx.TIME_TO_INT\n",
    "X_da = da.loc[:,:,hps.features + hps.calendar_features]\n",
    "#C_da = da.loc[:,:,hps.calendar_features]\n",
    "Y_da = da.loc[:,:,hps.responses]\n",
    "# compute the valid-data-mask - and account for priming in it\n",
    "V_da = (da.loc[:,:,'valid']\n",
    "        # fill small gaps\n",
    "        .ffill( dim='time', limit=13*10 ) \n",
    "        # extend out leading nulls by the priming factor\n",
    "        .rolling( time=hps.priming, min_periods=hps.priming ).mean() \n",
    "        # keep only the valid data for each symbol\n",
    "        .notnull() ) \n",
    "V_da = (V_da * da.loc[:,:,'valid'].fillna(0)).broadcast_like( Y_da )\n",
    "# the weight matrix --- to do.\n",
    "W_da = V_da \n",
    "\n",
    "if hps.data_standardize:\n",
    "    X_da = (X_da / X_da.std( dim=['time', 'symbol'] ) )\n",
    "    Y_da = (Y_da / Y_da.std( dim=['time', 'symbol'] ) )\n",
    "X_da = X_da.fillna( 0 )\n",
    "Y_da = Y_da.fillna( 0 )\n",
    "\n",
    "# Build the template masker and split into trg, tst, and validation\n",
    "# The priming shouls be enough to account for any lookahead issues.\n",
    "masker = mxtr.PanelMasker( \n",
    "    data=da, priming=hps.priming, \n",
    "    num_its=None,  trg_mask_size=None,  \n",
    "    tst_split=mx.SplitData.time_cutoff, tst_frac=0.20, tst_mask_size=None,    \n",
    "    val_split=mx.SplitData.time_cutoff, val_frac=0.20, val_mask_size=None,\n",
    ") \n",
    "masker.restrict( da )    \n",
    "trg_mask= masker.full_mask( 'trg' )\n",
    "val_mask= masker.full_mask( 'val' )\n",
    "tst_mask = masker.full_mask( 'tst' )\n",
    "assert len(set(trg_mask.values).union( set( tst_mask.values)).union( set(val_mask.values))) == len(da) - hps.priming\n",
    "disp(\n",
    "    'trg start, end', da[trg_mask].time.min().values, da[trg_mask].time.max().values, \n",
    "    'trg_frac=', len(trg_mask) / len(da),   \n",
    "    '\\nval start, end', da[val_mask].time.min().values, da[val_mask].time.max().values, \n",
    "    'val_frac=', len(val_mask) / len(da), \n",
    "    '\\ntst start, end', da[tst_mask].time.min().values, da[tst_mask].time.max().values, \n",
    "    'tst_frac=', len(tst_mask) / len(da),\n",
    ")\n",
    "\n",
    "# the training + validation dataset\n",
    "full_dataset = mxtr.TensorDataset(     \n",
    "    T_da.to_tensor(  ), # do not change this dtype - otherwise you lose precision\n",
    "    X_da.to_tensor( dtype=dtype ),     \n",
    "    Y_da.to_tensor( dtype=dtype ),  \n",
    "    W_da.to_tensor( dtype=dtype ),  \n",
    ")\n",
    "trg_dataset = mxtr.TensorDataset(     \n",
    "    T_da[trg_mask].to_tensor(  ), \n",
    "    X_da[trg_mask].to_tensor( dtype=dtype ),     \n",
    "    Y_da[trg_mask].to_tensor( dtype=dtype ),  \n",
    "    W_da[trg_mask].to_tensor( dtype=dtype ),  \n",
    ")\n",
    "val_dataset = mxtr.TensorDataset(     \n",
    "    T_da[val_mask].to_tensor(  ), \n",
    "    X_da[val_mask].to_tensor( dtype=dtype ),     \n",
    "    Y_da[val_mask].to_tensor( dtype=dtype ),  \n",
    "    W_da[val_mask].to_tensor( dtype=dtype ),  \n",
    ")\n",
    "tst_dataset  = mxtr.TensorDataset(     \n",
    "    T_da[tst_mask].to_tensor(  ), \n",
    "    X_da[tst_mask].to_tensor( dtype=dtype ),     \n",
    "    Y_da[tst_mask].to_tensor( dtype=dtype ),  \n",
    "    W_da[tst_mask].to_tensor( dtype=dtype ),  \n",
    ")\n",
    "\n",
    "if False:\n",
    "    Y_da.std( dim=['time']).to_dataframe_mx().hist(bins=100)\n",
    "    figure(figsize=(20,20))\n",
    "    for i, c in enumerate( X_da.features.values ):\n",
    "        ax = subplot( 5, 5, i+1 )\n",
    "        X_da.loc[:,:,c].std( dim=['time']).to_series().hist(bins=100, ax=ax )\n",
    "        ax.set_title( c )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# -- setup the model --\n",
    "mxtr = mxtr.reload()\n",
    "from research.etf_technical import model_v1 as mdl; reload(mdl)\n",
    "mx.Config.start()\n",
    "\n",
    "disp( mx.tensorboard.start( clear=True ) )\n",
    "disp( mx.tensorboard.start_server() )\n",
    "\n",
    "#learner = mxtr.Learner( optimizers=[],)    \n",
    "\n",
    "hps.chi2 = None\n",
    "hps.curvature = None\n",
    "hps.dirichlet = None\n",
    "hps.lasso = None\n",
    "hps.lipschitz = None\n",
    "hps.logdet = None\n",
    "hps.neumann = None\n",
    "hps.ridge = 1e-6\n",
    "hps.smooth_lasso = None\n",
    "hps.tv = 1e-6\n",
    "hps.error =  'mse'\n",
    "hps.activation = 'leaky_relu'\n",
    "hps.lookhead = 14 # 1 day lookahead\n",
    "error = mxtr.Error( metric=hps.error, kappa=1e-3 )\n",
    "util = mxtr.Error( metric='util', kappa=1 )\n",
    "pfo_sr = mxtr.Error( metric='pfo-sr', kappa=1e-6 )\n",
    "    \n",
    "mx.seed( 17 )\n",
    "model = mdl.FactorResnet( \n",
    "    num_features = len(hps.features+hps.calendar_features),\n",
    "    num_responses=len( hps.responses ),\n",
    "    L=13 * 3, \n",
    "    dropout=False,\n",
    "    batch_norm=False,\n",
    "    resnet_layers=5,\n",
    "    resnet_gating=False,\n",
    "    resnet_channels=15,\n",
    "    resnet_activation=hps.activation,\n",
    "    dilation=2,\n",
    "    monitor=True\n",
    ").initialize( 'normal' ).to( dtype ).to( device )\n",
    "\n",
    "disp( model.describe(), size=5 )\n",
    "\n",
    "def trg_loss( times, X, Y, W, mask=None, patience=1  ):\n",
    "    ''' the training loss composed of error + regularizations '''\n",
    "    # build up the regularization terms\n",
    "    model.train( True )\n",
    "    act_reg, Yhat = model.activation_regularization( X, chi2=hps.chi2, tv=hps.tv, curvature=hps.curvature, dirichlet=hps.dirichlet, neumann=hps.neumann  )\n",
    "    weight_reg = model.weight_regularization( ridge=hps.ridge, lasso=hps.lasso, smooth_lasso=hps.smooth_lasso, logdet=hps.logdet  )    \n",
    "    lip_reg = model.lipchitz_regularization( X=X, penalty=hps.lipschitz, lr=0.5, radius=1e-2, num_its=5, tensorboard=True  )[0] if hps.lipschitz else tr.tensor(0) \n",
    "    # the error and regularization terms\n",
    "    err = error( Yhat=Yhat, Y=Y, W=W, mask=mask )\n",
    "    reg = act_reg + weight_reg + lip_reg\n",
    "    Wsum = W[mask].sum() if mask is not None else W.sum()\n",
    "    # --- logging stuff---\n",
    "    if not hasattr( trg_loss, 'it_counter' ):\n",
    "        trg_loss.it_counter = 0\n",
    "        trg_loss.losses = []\n",
    "    trg_loss.losses.append( dict( err=err.item(), Wsum=Wsum.item(), batch_start=min( times.values ), reg=reg.item(), lip_reg=lip_reg.item(), weight_reg=weight_reg.item() ) )\n",
    "    if trg_loss.it_counter % patience == 0:        \n",
    "        try:\n",
    "            mx.tensorboard.logger.add_scalar( f'TrgLoss/error', err, trg_loss.it_counter  )        \n",
    "            mx.tensorboard.logger.add_scalar( f'TrgLoss/util', util( Yhat, Y, W, mask=mask) , trg_loss.it_counter  )        \n",
    "            mx.tensorboard.logger.add_scalar( f'TrgLoss/pfo_sr', pfo_sr( Yhat, Y, W, mask=mask)* np.sqrt( 252 ) , trg_loss.it_counter  )        \n",
    "            mx.tensorboard.logger.add_scalars( f'TrgLoss/regularizations', dict(total=reg, weight=weight_reg, act=act_reg, lip=lip_reg ), trg_loss.it_counter )        \n",
    "            mx.tensorboard.logger.add_scalars( f'TrgLoss/minibatch', dict( batch_start=times.values.min(), mask_start=times[mask].values.min() ), trg_loss.it_counter  )            \n",
    "            mx.tensorboard.logger.add_scalars( f'TrgLoss/sizes', dict( Wsum=Wsum, mb_size=W[mask].sum() ), trg_loss.it_counter  )                                 \n",
    "            #mx.tensorboard.logger.add_scalar( f'TrgLoss/unweighted_err', myerr, trg_loss.it_counter  )                        \n",
    "            mx.tensorboard.logger.add_text( \n",
    "                f'TrgLoss/minibatch', \n",
    "                (f'batch_start={str( pd.to_datetime( min( times.values ) * mx.TIME_TO_INT ) )} '\n",
    "                 f'mask_start={str( pd.to_datetime( min( times[mask].values ) * mx.TIME_TO_INT ) )}'), \n",
    "                trg_loss.it_counter  \n",
    "            )            \n",
    "        except Exception as e:\n",
    "            warnings.warn( f'tensorboard failure {e} \\n SKIPPING' )\n",
    "    trg_loss.it_counter += 1\n",
    "    return err + reg\n",
    "\n",
    "def val_loss( times, X, Y, W, mask=None, patience=10 ):\n",
    "    model.train( False )\n",
    "    Yhat = model( X )    \n",
    "    err = error( Yhat=Yhat, Y=Y, W=W, mask=mask )\n",
    "    if not hasattr( val_loss, 'it_counter' ):\n",
    "        val_loss.it_counter = 0\n",
    "    if val_loss.it_counter % patience == 0:\n",
    "        mx.tensorboard.logger.add_scalar( f'ValLoss/error', err, val_loss.it_counter  )        \n",
    "        mx.tensorboard.logger.add_scalar( f'ValLoss/util', util( Yhat, Y, W, mask=mask) , val_loss.it_counter  )        \n",
    "        mx.tensorboard.logger.add_scalar( f'ValLoss/pfo_sr', pfo_sr( Yhat, Y, W, mask=mask)* np.sqrt( 252 ) , val_loss.it_counter  )        \n",
    "        mx.tensorboard.logger.add_scalars( f'ValLoss/minibatch', dict( batch_start=times.values.min(), mask_start=times[mask].values.min() ), val_loss.it_counter  )            \n",
    "        mx.tensorboard.logger.add_scalars( f'ValLoss/sizes', dict( Wsum=W.sum(), mb_size=W[mask].sum() ), val_loss.it_counter  )                                 \n",
    "        # with tr.enable_grad():\n",
    "        #     lip_reg, _ = model.lipchitz_regularization( X, penalty=1, tensorboard=True, num_its=50 )\n",
    "        # mx.tensorboard.logger.add_scalar( f'ValLoss/lip_reg', lip_reg, val_loss.it_counter  )        \n",
    "    val_loss.it_counter += 1\n",
    "    return err\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# -setup the learner ---\n",
    "############################\n",
    "mxtr = mxtr.reload()\n",
    "from research.etf_technical import model_v1 as mdl; reload(mdl)\n",
    "mx.Config.start()\n",
    "\n",
    "disp( mx.tensorboard.start( clear=True ) )\n",
    "disp( mx.tensorboard.start_server() )\n",
    "\n",
    "opt = mxtr.FAdam( params=model.parameters(), lr=1e-3, betas=( 0.9, 0.99 ), tensorboard=False ) # mx.Resolve.halflife( 10)['decay'], mx.Resolve.halflife( 20 )['decay'] ) )\n",
    "# lr_scheduler = mxtr.CycleAdamLR( optimizer=opt, min_lr=1e-5, max_lr=1e-4, min_betas=(0.6,0.8), max_betas=(0.9, 0.9), T_max=200 )\n",
    "# lr_scheduler = mxtr.WarmupAdamLR( optimizer=opt, min_lr=1e-5, max_lr=1e-3, min_betas=(0.9,0.9), max_betas=(0.9, 0.99), T_max=500 )\n",
    "learner = mxtr.Learner(\n",
    "    optimizers=opt,\n",
    "    trg_loss=trg_loss,\n",
    "    val_loss=val_loss,    \n",
    "    max_its=5000,\n",
    "    device=device,\n",
    "    callbacks=[\n",
    "        mxtr.callbacks.LossLogger( patience=1 ),\n",
    "        mxtr.callbacks.ParameterLogger( named_params=model.named_parameters(), patience=10, grad_autocorr=True ),\n",
    "        mxtr.callbacks.ParameterEma( halflife=10, patience=5 )\n",
    "        # mxtr.callbacks.ParameterMacd( hl_fast=10, hl_slow=100, patience=5, rewind_factor=None  ),\n",
    "        #mxtr.callbacks.ModelSnapshots( model=model, period=100, on_minimum=True, val_halflife=10 )\n",
    "        #mxtr.calllbacks.ClipGradients( signize=False, clip=None, quantile=None, winlen=100 ),\n",
    "        # mxtr.callbacks.LRSchedule( lr_scheduler ),\n",
    "    ]    \n",
    ")\n",
    "disp( 'learner hash ', learner.hash(), h=4)\n",
    "disp( hps )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.seed( 8 )\n",
    "model.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# --- evaluate the untrained model ----\n",
    "%inline\n",
    "# with mx.timer( 'predicting '):\n",
    "#     Yh = model.to('cpu')( X_da.to_tensor( dtype=dtype, device='cpu' ) )\n",
    "#     model.to( device )\n",
    "# Yh_da = Yh.to_dataarray( Y_da )\n",
    "\n",
    "trg_start, trg_end = pd.to_datetime( (trg_dataset[0:1][0].item() * mx.TIME_TO_INT, trg_dataset[-1:][0].item() * mx.TIME_TO_INT)  )\n",
    "val_start, val_end = pd.to_datetime( (val_dataset[0:1][0].item() * mx.TIME_TO_INT, val_dataset[-1:][0].item() * mx.TIME_TO_INT)  )\n",
    "trg_times = da.loc[trg_start:trg_end].time\n",
    "val_times = da.loc[val_start:val_end].time\n",
    "tst_times = da[tst_mask].time\n",
    "\n",
    "fig = figure( figsize=(10, 10 ) )\n",
    "\n",
    "with mx.timer( 'regressing' ):    \n",
    "    reg = mx.Regression( autocorr=[1, 13, 26] )\n",
    "    rms = []\n",
    "    #for i, (smp, mask) in pbar( enumerate( [('trg', trg_mask.values), ('val', val_mask.values), ('tst',tst_mask.values), ('full',range(len(Y_da))) ] ) ):\n",
    "    for i, (smp, times) in pbar( enumerate( [('trg', trg_times), ('val', val_times), ('tst',tst_times), ('full', Y_da.time ) ] ) ):\n",
    "        rm = reg.regress( Yh_da.loc[times], Y_da.loc[times], W_da.loc[times] )    \n",
    "        ax = subplot( 2,2, i+1 )\n",
    "        ax.plot( rm.pop('pnl').cumsum() )\n",
    "        ax.set_title( smp, fontsize=15 )\n",
    "        hr = rm.pop('hitRate')\n",
    "        rm['sample'] = smp\n",
    "        rms.append( rm )\n",
    "disp( 'Untrained model ', h= 1)\n",
    "disp( pd.DataFrame( rms ).set_index('sample') )\n",
    "disp( fig )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# --- do the fitting ---\n",
    "#build up the trianing and validation dataloaders\n",
    "hps.batch_size = 2 * hps.priming  # yrs worth of batches\n",
    "tds = trg_dataset #mxtr.TensorDataset( *trg_dataset[-20000:] )\n",
    "vds = val_dataset #val_dataset # mxtr.TensorDataset( *val_dataset[5000:] )\n",
    "\n",
    "trg_dataloader = mxtr.DataLoader( tds, batch_sampler=mxtr.PanelSampler( tds, batch_size=None ), pin_memory=True )\n",
    "val_dataloader = mxtr.DataLoader( vds, batch_sampler=mxtr.PanelSampler( vds, batch_size=None ), pin_memory=True )\n",
    "\n",
    "# build up the training and valdiation maskers\n",
    "trg_masker = masker.clone( trg_mask_size=None, num_its=1 )\n",
    "val_masker = masker.clone( val_mask_size=None, num_its=1 ) \n",
    "\n",
    "for it in pbar( learner.fit_iterator( data=trg_dataloader, masker=trg_masker, val_data=val_dataloader, val_masker=val_masker ) ) :\n",
    "    continue\n",
    "#     if it % 10 == 0:\n",
    "#         tloss = val_loss( *[m.to(device) for m in tst_dataset[:]]  )\n",
    "#         print( 'testing  loss', tloss )\n",
    "#     if it % 500 == 0:\n",
    "#         print( model.hash() )\n",
    "#         disp( learner.profiles.sort_values( 'duration_mean', ascending=False ) )\n",
    "\n",
    "# # run a explicit traning loop\n",
    "# opt = mxtr.FAdam( params=model.parameters(), lr=1e-3, betas=( 0.9, 0.99 ), tensorboard=False ) # mx.Resolve.halflife( 10)['decay'], mx.Resolve.halflife( 20 )['decay'] ) )\n",
    "# it  = 0\n",
    "# for tmb, vmb in pbar( zip( trg_dataloader, val_dataloader ) ):\n",
    "#     it += 1\n",
    "#     tmb = [m.to(device) for m in tmb]\n",
    "#     vmb = [m.to(device) for m in vmb]\n",
    "#     trg_masker.restrict( tmb[0] )\n",
    "#     val_masker.restrict( vmb[0] )\n",
    "#     trg_mask_iterator = trg_masker.mask_iterator('trg')\n",
    "#     val_mask_iterator = val_masker.mask_iterator('val')\n",
    "#     opt.zero_grad()\n",
    "#     trg_mask = next( trg_mask_iterator )\n",
    "#     val_mask = next( val_mask_iterator )\n",
    "#     tloss = trg_loss( *tmb, mask=trg_mask )\n",
    "#     tloss.backward()\n",
    "#     opt.step()\n",
    "#     vloss = val_loss( *vmb, mask=val_mask )\n",
    "#     if it % 10 == 0:\n",
    "#         tloss = val_loss( *[m.to(device) for m in tst_dataset[:]]  )\n",
    "#         print( 'testing  loss', tloss )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# --- evaluate the trained model ----\n",
    "%inline\n",
    "with mx.timer( 'predicting '):\n",
    "    Yh = model.to('cpu')( X_da.to_tensor( dtype=dtype, device='cpu' ) )\n",
    "    model.to(device)\n",
    "Yh_da = Yh.to_dataarray( Y_da )\n",
    "\n",
    "trg_start, trg_end = pd.to_datetime( (trg_dataset[0:1][0].item() * mx.TIME_TO_INT, trg_dataset[-1:][0].item() * mx.TIME_TO_INT)  )\n",
    "val_start, val_end = pd.to_datetime( (val_dataset[0:1][0].item() * mx.TIME_TO_INT, val_dataset[-1:][0].item() * mx.TIME_TO_INT)  )\n",
    "trg_times = da.loc[trg_start:trg_end].time\n",
    "val_times = da.loc[val_start:val_end].time\n",
    "tst_times = da[tst_mask].time\n",
    "\n",
    "fig = figure( figsize=(10, 10 ) )\n",
    "\n",
    "with mx.timer( 'regressing' ):    \n",
    "    reg = mx.Regression( autocorr=[1, 13, 26] )\n",
    "    rms = []\n",
    "    #for i, (smp, mask) in pbar( enumerate( [('trg', trg_mask.values), ('val', val_mask.values), ('tst',tst_mask.values), ('full',range(len(Y_da))) ] ) ):\n",
    "    for i, (smp, times) in pbar( enumerate( [('trg', trg_times), ('val', val_times), ('tst',tst_times), ('full', Y_da.time ) ] ) ):\n",
    "        rm = reg.regress( Yh_da.loc[times], Y_da.loc[times], W_da.loc[times] )    \n",
    "        ax = subplot( 2,2, i+1 )\n",
    "        ax.plot( rm.pop('pnl').cumsum() )\n",
    "        ax.set_title( smp, fontsize=15 )\n",
    "        hr = rm.pop('hitRate')\n",
    "        rm['sample'] = smp\n",
    "        rms.append( rm )\n",
    "disp( 'Trained model ', h= 1)\n",
    "disp( pd.DataFrame( rms ).set_index('sample') )\n",
    "disp( fig )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_da.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot partial dependency\n",
    "##############################################\n",
    "%inline\n",
    "fig = figure( figsize=(20, 20 ) )\n",
    "for i in range(X_da.shape[-1]):\n",
    "    subplot( 4, 4, i+1 )\n",
    "    plot( X_da[:,:,i].to_series(), Yh_da.to_series() , '.', alpha=0.3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# run a grid of hyperparameters ...\n",
    "####################################\n",
    "\n",
    "# hyperparameters\n",
    "mx.Config.hps = fr.HPSet( \n",
    "  \n",
    ")\n",
    "mx.Config.code = ''\n",
    "\n",
    "# configs = []\n",
    "# for obs_weight in ['no_weight', 'dollar_weight', 'log_weight']:\n",
    "#     cfg = fr.Config.clone()\n",
    "#     cfg.hps = cfg.hps.clone( obs_weight=obs_weight )\n",
    "#     cfg.freeze()\n",
    "#     display_html( cfg )\n",
    "#     configs.append( cfg )\n",
    "# notebook = fr.Config.run_grid( configs, outfile=fr.Config.uri+'/../cadre_price_index.20200302 - weight_grid_results.ipynb', n_jobs=None )\n",
    "\n",
    "configs = []\n",
    "fr.Config.hps.obs_weight = 'log_weight'\n",
    "for cbsa_factors in [5, 8, 10]:\n",
    "    cfg = fr.Config.clone()\n",
    "    cfg.hps = cfg.hps.clone( cbsa_factors=cbsa_factors )\n",
    "    cfg.freeze()\n",
    "    #display_html( cfg )\n",
    "    configs.append( cfg )\n",
    "notebook = fr.Config.run_grid( configs, outfile=fr.Config.uri+'/../grid_results.ipynb', n_jobs=None )\n",
    "\n",
    "\n",
    "# fr.Config.hps.obs_weight = 'log_weight'\n",
    "# fr.Config.hps.cbsa_factors = 8\n",
    "# configs = []\n",
    "# for tv in [1, 1e-1, 1e-2, 1e-3]:\n",
    "#     cfg = fr.Config.clone()\n",
    "#     cfg.hps = cfg.hps.clone( obs_weight=obs_weight )\n",
    "#     cfg.freeze()\n",
    "#     #display_html( cfg )\n",
    "#     configs.append( cfg )\n",
    "# notebook = fr.Config.run_grid( configs, outfile=fr.Config.uri+'/../cadre_price_index.20200302 - tv_grid_results.ipynb', n_jobs=None )\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
